# RecipeCraft - Recipe Retrieval with RAG, FAISS & Ollama

**RecipeCraft** is an AI-powered recipe retrieval system that combines modern vector search with large language models (LLMs). Built with FAISS indexing, Sentence Transformers, and the Ollama framework, it enables semantic recipe search using natural language prompts.

---

## ğŸ“ Project Structure

```
RECIPE-CRAFT
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ config_model_client.yaml   # Configuration for the LLM model client
â”‚   â””â”€â”€ config_rag.yaml            # Main configuration for embedding, index, and paths
â”œâ”€â”€ data/                          # Initial recipe dataset in JSON format
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ llm/
â”‚   â”‚   â””â”€â”€ model_client.py        # Logic for interacting with the Ollama-hosted LLM
â”‚   â”œâ”€â”€ rag/
â”‚   â”‚   â””â”€â”€ retriever.py           # RecipeRetriever class: embedding, indexing, and retrieval
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ utils_file.py          # File loading/saving utilities (JSON, YAML)
â”œâ”€â”€ app.py                         # FastAPI application entrypoint
â”œâ”€â”€ docker-compose.yml             # Docker Compose configuration
â”œâ”€â”€ Dockerfile                     # Docker image definition
â”œâ”€â”€ requirements.txt               # Python dependencies
â””â”€â”€ README.md                      # This file
```

---

## ğŸš€ How to Launch

### 1. Clone the repository
```bash
git clone https://github.com/slavastar/recipe-craft.git
cd recipe-craft
```

### 2. Set up virtual environment (optional for local dev)
```bash
python3 -m venv venv
source venv/bin/activate
```

### 3. Launch the project using Docker
```bash
docker compose up --build
```

- This will build the image and start the application.
- **Note:** Image construction and container initialization may take several minutes.
- Once running, access the application at: [http://localhost:8000/docs](http://localhost:8000/docs)

### 3. Interact with the API
- Visit the `/ask` endpoint in Swagger UI
- Enter a natural language query (e.g. "Show me a vegan pasta recipe with spinach")
- The system may take several dozen seconds to respond depending on model initialization

---

## ğŸ§  Technologies Used

| Component        | Description                                    |
|------------------|------------------------------------------------|
| **Embedding Model** | `all-MiniLM-L6-v2` via Sentence Transformers |
| **LLM**          | `phi3` hosted using [Ollama](https://ollama.com) |
| **Vector Storage** | FAISS index of type `IVFPQ`                   |
| **Containerization** | Docker + Docker Compose                     |
| **API**          | FastAPI + Swagger for interactive exploration  |

---

## ğŸ“¦ Model Storage

The FAISS index and processed recipes are saved using the following config keys:

```yaml
recipe_storage: storage/recipe.json
faiss_storage: storage/recipe.index
```

These paths are set in `config/config_rag.yaml`.

---

## ğŸ“š Resources

- ğŸ“– Original recipe dataset: [EightPortions Recipe Dataset](https://eightportions.com/datasets/Recipes/)
- ğŸ”— Ollama LLM runner: [https://ollama.com](https://ollama.com)
- ğŸ“¦ FAISS documentation: [https://faiss.ai](https://faiss.ai)
- ğŸ¤— SentenceTransformers: [https://www.sbert.net](https://www.sbert.net)

---

## ğŸ› ï¸ Notes

- The first-time image build will download models and tools which may take a few minutes.
- Model initialization during the first prompt may also delay response by 10â€“40 seconds.
- FAISS `IVFPQ` index requires training and may not respond until trained.

---